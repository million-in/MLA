{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxqpMetPTZw//30S6LrQGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/million-in/MLA/blob/main/Multi_Head_Latent_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is an implementation of Multi head Latent Attention. A solution to dealing with KV cache.\n",
        "\n",
        "This implementation follows just one single attention block for simplicty. and we use GPT-3 configuration. D_MODEL, D_K.\n",
        "\n",
        "Multi head Latent Attention is mainly projecting hidden state of tokens into a row rank dimensional space or, a latent representation. wich is compressed. And then having to learners that will build Key and Value outputs for the tokens from the compressed matrix.\n",
        "\n",
        "MLA simply comes down two three steps,\n",
        "\n",
        "1. Low rank key-value joint compression into a latent representation.\n",
        "2. Applying decouple RoPe\n",
        "3. Backward projection to a higher dimension space for key and values from the latent representation.\n"
      ],
      "metadata": {
        "id": "rGT0I2eRn8Nt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iHbxbUAvvRm7"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from flax.core import freeze, unfreeze"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WiWZioPWxGr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define and initialize the embedding layer\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    num_embeddings: int\n",
        "    features: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.embedding = nn.Embed(num_embeddings=self.num_embeddings, features=self.features)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "# Define model\n",
        "D_MODEL = 12288  # Embedding dimension\n",
        "NUM_HEADS = 96   # Number of attention heads\n",
        "D_K = D_MODEL // NUM_HEADS  # Head dimension\n",
        "\n",
        "model = EmbeddingLayer(num_embeddings=50257, features=D_MODEL)\n",
        "\n",
        "# Initialize the model\n",
        "key = jax.random.PRNGKey(0)\n",
        "params = model.init(key, jnp.array([0]))  # Initialize with a dummy input\n",
        "\n",
        "# Print initialized embedding weights\n",
        "print(\"Embedding layer initialized:\")\n",
        "print(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyjBR9VBvXc9",
        "outputId": "0aaa8b65-4fe5-43b6-80e7-d1754a98dfc0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding layer initialized:\n",
            "{'params': {'embedding': {'embedding': Array([[ 6.6024400e-03,  1.5081277e-02, -1.1323430e-02, ...,\n",
            "        -4.9526279e-05,  4.3605505e-03, -2.2229243e-03],\n",
            "       [-8.1163282e-03,  6.7466344e-03, -3.4407864e-03, ...,\n",
            "        -9.2547555e-03,  8.8060035e-05,  3.5618185e-03],\n",
            "       [-7.9459362e-03, -8.6927693e-03,  9.2485854e-03, ...,\n",
            "        -8.3842678e-03,  1.8406912e-03, -5.2417803e-04],\n",
            "       ...,\n",
            "       [-4.3475558e-03,  4.9987296e-03,  6.4304932e-03, ...,\n",
            "         2.5832008e-03,  2.6981889e-03, -3.2927021e-03],\n",
            "       [ 7.2161956e-03,  1.2845425e-02, -9.2969723e-03, ...,\n",
            "        -4.9834945e-03, -2.9959911e-03,  9.5217573e-03],\n",
            "       [-1.1894217e-02,  5.3528803e-03,  1.3001672e-02, ...,\n",
            "         5.9268093e-03, -2.5876894e-04, -4.3124328e-03]], dtype=float32)}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test the model with a dummy set of tokens\n",
        "dummy_tokens = jnp.arange(1024) % 50257  # Generate a sequence of dummy tokens\n",
        "embedded_tokens = model.apply(params, dummy_tokens)\n",
        "\n",
        "print(\"Test output (embedded tokens shape):\", embedded_tokens.shape)\n",
        "print(\"First embedded vector:\", embedded_tokens[0])\n",
        "print(\"Last embedded vector:\", embedded_tokens[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvHZLfvxvj4C",
        "outputId": "7baf08d9-e913-43c9-e96e-dc2909be4793"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test output (embedded tokens shape): (1024, 12288)\n",
            "First embedded vector: [ 6.6024400e-03  1.5081277e-02 -1.1323430e-02 ... -4.9526279e-05\n",
            "  4.3605505e-03 -2.2229243e-03]\n",
            "Last embedded vector: [-0.00201211 -0.02040401  0.01133402 ...  0.00343121 -0.00958229\n",
            " -0.01465543]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Layer Normalization with learnable parameters\n",
        "def layer_norm(x, gamma, beta, epsilon=1e-5):\n",
        "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
        "    variance = jnp.var(x, axis=-1, keepdims=True)\n",
        "    normalized_x = (x - mean) / jnp.sqrt(variance + epsilon)\n",
        "    return normalized_x * gamma + beta\n",
        "\n",
        "# Initialize learnable parameters\n",
        "gamma = jnp.ones((1, D_MODEL))  # Scale parameter\n",
        "beta = jnp.zeros((1, D_MODEL))  # Shift parameter\n",
        "\n",
        "normalized_embeddings = layer_norm(embedded_tokens, gamma, beta)\n",
        "\n",
        "print(\"First normalized vector:\", normalized_embeddings[0])\n",
        "print(\"Last normalized vector:\", normalized_embeddings[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHNT4LXqv9f3",
        "outputId": "060a8864-259b-4f49-a334-c671b60a40f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First normalized vector: [ 0.7045637   1.5899293  -1.1672673  ...  0.00996106  0.47046414\n",
            " -0.21698657]\n",
            "Last normalized vector: [-0.20973752 -2.1317196   1.1849535  ...  0.3590983  -1.0008328\n",
            " -1.5309834 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6fOhUqPnqASG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now here we apply the first phase of our MLA. Projecting the hidden states (here normalized embeddings are acting as hidden states since this is the first layer) into a row rank dimension space. The dimension as you can see is 4 times smaller than the dimension of the model ie REDUCED_D_MODEL = D_MODEL // 4.\n",
        "\n",
        "We do this by introducing a new downward projection matrix W(c) as parameters. to form C\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XRlez_6ZrSSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now here we implement mla starting from only cell 4 of layer norm\n",
        "# Project normalized output to a lower-rank latent space for compression\n",
        "\n",
        "def project_to_low_rank(x, projection_matrix):\n",
        "    \"\"\"Projects the input to a lower-rank latent space.\n",
        "\n",
        "    Args:\n",
        "      x: The input tensor.\n",
        "      projection_matrix: The matrix used for projection.\n",
        "\n",
        "    Returns:\n",
        "      The projected tensor.\n",
        "    \"\"\"\n",
        "    return jnp.matmul(x, projection_matrix)\n",
        "\n",
        "# Define reduced dimension\n",
        "REDUCED_D_MODEL = D_MODEL // 4\n",
        "\n",
        "# Initialize the projection matrix\n",
        "projection_key = jax.random.PRNGKey(1)\n",
        "projection_matrix = jax.random.normal(projection_key, (D_MODEL, REDUCED_D_MODEL)) * jnp.sqrt(2 / (D_MODEL + REDUCED_D_MODEL)) #Glorot initialization\n",
        "\n",
        "\n",
        "# Project the normalized embeddings\n",
        "projected_embeddings = project_to_low_rank(normalized_embeddings, projection_matrix)\n",
        "\n",
        "# Print the shape of the output\n",
        "print(\"Shape of projected embeddings:\", projected_embeddings.shape)\n",
        "print(\"First projected vector:\", projected_embeddings[0])\n",
        "print(\"Last projected vector:\", projected_embeddings[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro-x1VKEGqAy",
        "outputId": "af4b7d7d-3e19-4b61-da8a-8ff6f8bf073a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of projected embeddings: (1024, 3072)\n",
            "First projected vector: [ 2.8534365  -0.5653596   0.91363555 ... -0.28176805 -1.0861151\n",
            " -0.5680051 ]\n",
            "Last projected vector: [-0.99326026 -0.58047986  0.37042296 ...  2.9131997  -0.23583116\n",
            " -1.5297287 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we Apply RoPe on half of the latent representation. This is because we definetly can not apply RoPe on the whole latent vector because that would encode only position information, while we will also use this to compute value"
      ],
      "metadata": {
        "id": "wOfOvh3Wsi8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply RoPE to a portion of the projected embeddings\n",
        "\n",
        "def apply_rope(x, freqs_cis):\n",
        "    \"\"\"Applies Rotary Positional Embeddings (RoPE) to the input.\n",
        "\n",
        "    Args:\n",
        "        x: The input tensor to apply RoPE to.  Shape: (seq_len, dim)\n",
        "        freqs_cis: Precomputed complex exponentials. Shape: (seq_len, dim // 2)\n",
        "                   This contains the values  exp(i * m * theta_t)  for\n",
        "                   positions m and frequencies 1 / (10000^(2t/d)).\n",
        "    Returns:\n",
        "      The input tensor with RoPE applied.\n",
        "    \"\"\"\n",
        "    x_complex = x.astype(jnp.complex64)\n",
        "    x_rotated = x_complex * freqs_cis\n",
        "    return x_rotated.real.astype(x.dtype)  # Return real part, same dtype as input\n",
        "\n",
        "def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):\n",
        "    \"\"\"Precomputes the complex exponentials for RoPE.\n",
        "\n",
        "    Args:\n",
        "        dim: The dimension of the embeddings (must be even for RoPE).\n",
        "        seq_len: The maximum sequence length.\n",
        "        theta: The base for the geometric progression.\n",
        "\n",
        "    Returns:\n",
        "        freqs_cis: Complex exponentials, shape (seq_len, dim // 2).\n",
        "    \"\"\"\n",
        "    freqs = 1.0 / (theta ** (jnp.arange(0, dim, 2)[: (dim // 2)].astype(jnp.float32) / dim))\n",
        "    t = jnp.arange(seq_len)  # type: ignore\n",
        "    freqs = jnp.outer(t, freqs)  # type: ignore\n",
        "    freqs_cis = jnp.exp(1j * freqs)\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "# 1. Split the projected embeddings\n",
        "split_index = REDUCED_D_MODEL // 2\n",
        "content_embeddings = projected_embeddings[:, :split_index]\n",
        "positional_embeddings = projected_embeddings[:, split_index:]\n",
        "\n",
        "# 2. Precompute freqs_cis for RoPE\n",
        "seq_len = projected_embeddings.shape[0]  # Get the sequence length\n",
        "freqs_cis = precompute_freqs_cis(positional_embeddings.shape[1] * 2, seq_len)  # *2 to account the split\n",
        "freqs_cis = freqs_cis[:,:positional_embeddings.shape[1]] #new line added\n",
        "\n",
        "# 3. Apply RoPE to the positional embeddings\n",
        "roped_positional_embeddings = apply_rope(positional_embeddings, freqs_cis)\n",
        "\n",
        "# 4. Combine the content and RoPE-transformed positional embeddings\n",
        "combined_embeddings = jnp.concatenate([content_embeddings, roped_positional_embeddings], axis=-1)\n",
        "\n",
        "print(\"Shape of content embeddings:\", content_embeddings.shape)\n",
        "print(\"Shape of original positional embeddings:\", positional_embeddings.shape)\n",
        "print(\"Shape of RoPE'd positional embeddings:\", roped_positional_embeddings.shape)\n",
        "print(\"Shape of combined embeddings:\", combined_embeddings.shape)\n",
        "print(\"First combined vector:\", combined_embeddings[0])\n",
        "print(\"Last combined vector:\", combined_embeddings[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7uuiTWSi4KN",
        "outputId": "68c4d06c-6cda-4fdd-fa71-d6b0052c5027"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of content embeddings: (1024, 1536)\n",
            "Shape of original positional embeddings: (1024, 1536)\n",
            "Shape of RoPE'd positional embeddings: (1024, 1536)\n",
            "Shape of combined embeddings: (1024, 3072)\n",
            "First combined vector: [ 2.8534365  -0.5653596   0.91363555 ... -0.28176805 -1.0861151\n",
            " -0.5680051 ]\n",
            "Last combined vector: [-0.99326026 -0.58047986  0.37042296 ...  2.8974118  -0.23456831\n",
            " -1.5216347 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Multi-Head Attention Matrices\n",
        "\n",
        "def initialize_attention_head(key, d_model, reduced_d_model, d_k):\n",
        "    \"\"\"Initializes the projection matrices for a single attention head.\n",
        "\n",
        "    Args:\n",
        "        key: A JAX PRNGKey for random number generation.\n",
        "        d_model: The input dimension of the model.\n",
        "        reduced_d_model: The dimension of reduced space (from previous cell)\n",
        "        d_k: The dimension of the key, query, and value projections.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the initialized query, key, and value\n",
        "        projection matrices for the head.\n",
        "    \"\"\"\n",
        "    k_q, k_k, k_v = jax.random.split(key, 3)  # Split key for each matrix\n",
        "\n",
        "    # Xavier/Glorot initialization for each matrix\n",
        "    query_projection = jax.random.normal(k_q, (d_model, d_k)) * jnp.sqrt(2 / (d_model + d_k))\n",
        "    key_projection = jax.random.normal(k_k, (reduced_d_model, d_k)) * jnp.sqrt(2 / (reduced_d_model + d_k))\n",
        "    value_projection = jax.random.normal(k_v, (reduced_d_model, d_k)) * jnp.sqrt(2 / (reduced_d_model + d_k))\n",
        "\n",
        "    return {\n",
        "        \"query\": query_projection,\n",
        "        \"key\": key_projection,\n",
        "        \"value\": value_projection,\n",
        "    }\n",
        "\n",
        "\n",
        "# Initialize all attention heads\n",
        "attention_heads = []\n",
        "main_key = jax.random.PRNGKey(2)  # New key for the attention heads\n",
        "\n",
        "for i in range(NUM_HEADS):\n",
        "    head_key = jax.random.fold_in(main_key, i)  # Generate a unique key for each head\n",
        "    head_params = initialize_attention_head(head_key, D_MODEL, REDUCED_D_MODEL, D_K)\n",
        "    attention_heads.append(head_params)\n",
        "\n",
        "# Print the shapes of the matrices for a single head (since they are all the same)\n",
        "print(f\"Head (Shapes are the same for all heads):\")\n",
        "print(f\"  Query Projection Shape: {attention_heads[0]['query'].shape}\")\n",
        "print(f\"  Key Projection Shape: {attention_heads[0]['key'].shape}\")\n",
        "print(f\"  Value Projection Shape: {attention_heads[0]['value'].shape}\")\n",
        "print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzhwdzeKJ4Qt",
        "outputId": "5fa6011f-dcd7-4790-8702-b32b4dbb0981"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head (Shapes are the same for all heads):\n",
            "  Query Projection Shape: (12288, 128)\n",
            "  Key Projection Shape: (3072, 128)\n",
            "  Value Projection Shape: (3072, 128)\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since we have a compresedd representation in a low dimensiona space and we then have applied a decouple RoPe to include positional encodings. The question is how do we calculate attention.\n",
        "\n",
        "Here we apply the last step of decompression. First we project the normalized input into query projection matrix and apply rope. yeap for query we definetly dont change anything. we compute it as normal.\n",
        "\n",
        "Then for value and Key we introduce new learner W(k) and W(v). where these matrices learn to form a key and value vector for each token from the latent vector C.\n",
        "\n",
        "Remember how we split C to apply decouple loss if you want you can calcuate value vectors from the part of C without just positional embedings as we dont need them in Value vectors.\n",
        "\n",
        "Now thats MLA in full now whats next is just computing attention on the constructed Keys and Value vectors."
      ],
      "metadata": {
        "id": "Gk1C0a9btypR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Query, Key, and Value Vectors with RoPE (Using Provided RoPE Function - Query Only)\n",
        "\n",
        "def compute_attention_inputs(query_input, key_value_input, attention_heads):\n",
        "    \"\"\"Computes the query, key, and value vectors for all attention heads.\n",
        "\n",
        "    Args:\n",
        "        query_input: The input for query projection (normalized embeddings).\n",
        "        key_value_input: The input for key and value projection (combined embeddings).\n",
        "        attention_heads: A list of dictionaries, each containing the projection\n",
        "                         matrices for an attention head.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing lists of query, key, and value vectors for all heads.\n",
        "    \"\"\"\n",
        "    all_queries = []\n",
        "    all_keys = []\n",
        "    all_values = []\n",
        "\n",
        "    for head in attention_heads:\n",
        "        # Query projection\n",
        "        query = jnp.matmul(query_input, head['query'])\n",
        "\n",
        "        # Key and Value projections\n",
        "        key = jnp.matmul(key_value_input, head['key'])\n",
        "        value = jnp.matmul(key_value_input, head['value'])\n",
        "\n",
        "        all_queries.append(query)\n",
        "        all_keys.append(key)\n",
        "        all_values.append(value)\n",
        "\n",
        "    return all_queries, all_keys, all_values\n",
        "\n",
        "def apply_rope(x, print_transforms=False):\n",
        "    original_shape = x.shape\n",
        "    x = x.reshape(1, x.shape[0], 1, x.shape[1])\n",
        "    batch_size, seq_len, num_heads, d_k = x.shape\n",
        "\n",
        "\n",
        "    theta = 10000.0 ** (-jnp.arange(0, d_k, 2) / d_k)\n",
        "    pos = jnp.arange(seq_len)[:, None]\n",
        "\n",
        "    sin = jnp.sin(pos * theta)\n",
        "    cos = jnp.cos(pos * theta)\n",
        "\n",
        "\n",
        "    sin = sin[None, :, None, :]\n",
        "    cos = cos[None, :, None, :]\n",
        "\n",
        "    x_even, x_odd = x[..., ::2], x[..., 1::2]\n",
        "\n",
        "    if print_transforms:\n",
        "        print(\"\\nRoPE Transformations:\")\n",
        "        print(f\"Theta shape (frequencies): {theta.shape}\")\n",
        "        print(f\"First few theta values: {theta[:5]}\")\n",
        "\n",
        "\n",
        "        example_cos = cos[0, 0, 0]\n",
        "        example_sin = sin[0, 0, 0]\n",
        "        example_even = x_even[0, 0, 0]\n",
        "        example_odd = x_odd[0, 0, 0]\n",
        "\n",
        "\n",
        "        print(f\"\\nFirst position rotation values:\")\n",
        "        print(f\"cos values: {example_cos[:5]}\")\n",
        "        print(f\"sin values: {example_sin[:5]}\")\n",
        "        print(f\"\\nExample rotation for first vector pair:\")\n",
        "        print(f\"Original values (even, odd): ({example_even[0]}, {example_odd[0]})\")\n",
        "        print(f\"Rotated values: ({(example_even[0] * example_cos[0] - example_odd[0] * example_sin[0]).item()}, \"\n",
        "              f\"{(example_even[0] * example_sin[0] + example_odd[0] * example_cos[0]).item()})\")\n",
        "\n",
        "    x_rotated = jnp.stack([\n",
        "        x_even * cos - x_odd * sin,\n",
        "        x_even * sin + x_odd * cos\n",
        "    ], axis=-1)\n",
        "\n",
        "    return x_rotated.reshape(original_shape)  # Reshape back to original 2D\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compute the query, key, and value vectors (NO RoPE applied here)\n",
        "all_queries, all_keys, all_values = compute_attention_inputs(\n",
        "    normalized_embeddings, combined_embeddings, attention_heads\n",
        ")\n",
        "\n",
        "# Apply RoPE *only* to queries after projection\n",
        "all_queries_roped = [apply_rope(q) for q in all_queries]\n",
        "\n",
        "\n",
        "# Print shapes for verification (taking the first head as an example)\n",
        "print(\"Shapes for the first attention head:\")\n",
        "print(f\"  Query Shape (after RoPE): {all_queries_roped[0].shape}\")\n",
        "print(f\"  Key Shape: {all_keys[0].shape}\")  # No RoPE\n",
        "print(f\"  Value Shape: {all_values[0].shape}\") # No RoPE\n",
        "print(\"-\" * 20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO3v5b7-SR8v",
        "outputId": "f7341c98-e55f-4c90-9689-68d7369ba34c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes for the first attention head:\n",
            "  Query Shape (after RoPE): (1024, 128)\n",
            "  Key Shape: (1024, 128)\n",
            "  Value Shape: (1024, 128)\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Calculate Attention Weights and Apply to Values\n",
        "\n",
        "def calculate_attention(queries, keys, values, d_k):\n",
        "    \"\"\"Calculates the scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        queries: A list of query vectors for all heads.\n",
        "        keys: A list of key vectors for all heads.\n",
        "        values: A list of value vectors for all heads.\n",
        "        d_k: The dimension of the key/query vectors (per head).\n",
        "\n",
        "    Returns:\n",
        "        A list of attention-weighted value vectors for all heads.\n",
        "    \"\"\"\n",
        "    attention_outputs = []\n",
        "    for q, k, v in zip(queries, keys, values):\n",
        "        # Calculate attention weights.  (seq_len_q, d_k) @ (d_k, seq_len_k) -> (seq_len_q, seq_len_k)\n",
        "        attention_scores = jnp.matmul(q, k.transpose()) / jnp.sqrt(d_k)\n",
        "        attention_weights = jax.nn.softmax(attention_scores, axis=-1)  # Softmax over the last axis (seq_len_k)\n",
        "\n",
        "        # Apply attention weights to the values. (seq_len_q, seq_len_k) @ (seq_len_k, d_v) -> (seq_len_q, d_v)\n",
        "        attention_output = jnp.matmul(attention_weights, v)\n",
        "        attention_outputs.append(attention_output)\n",
        "    return attention_outputs\n",
        "\n",
        "\n",
        "# Calculate the attention outputs using the RoPE-transformed queries, and original keys/values\n",
        "attention_outputs = calculate_attention(all_queries_roped, all_keys, all_values, D_K)\n",
        "\n",
        "# Print the shape of the output for the first head\n",
        "print(\"Shape of attention output for the first head:\", attention_outputs[0].shape)\n",
        "# Print shapes for all heads\n",
        "print(\"Shapes of all attention outputs\", [o.shape for o in attention_outputs])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsPKLPB0kPbd",
        "outputId": "bff93a2e-7358-427f-eee6-157b2902e04d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attention output for the first head: (1024, 128)\n",
            "Shapes of all attention outputs [(1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128), (1024, 128)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Concatenate Attention Outputs (Debugging - No Projection)\n",
        "\n",
        "def concatenate_attention_outputs(attention_outputs):\n",
        "    \"\"\"Concatenates the attention outputs from all heads.\n",
        "\n",
        "    Args:\n",
        "        attention_outputs: A list of attention-weighted value vectors for all heads.\n",
        "\n",
        "    Returns:\n",
        "        The concatenated attention outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate the outputs along the head dimension.\n",
        "    # Each attention_output has shape (seq_len, d_k)\n",
        "    # After concatenation: (seq_len, num_heads * d_k)\n",
        "    concatenated_outputs = jnp.concatenate(attention_outputs, axis=-1)\n",
        "    return concatenated_outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Concatenate the attention outputs\n",
        "concatenated_attention = concatenate_attention_outputs(attention_outputs)\n",
        "\n",
        "print(\"Shape of concatenated attention outputs:\", concatenated_attention.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY1Yck-ytJfJ",
        "outputId": "da50711d-048e-453a-d023-8af8b5ac383e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of concatenated attention outputs: (1024, 12288)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Initialize Final Projection Matrix (Corrected Variable Name)\n",
        "\n",
        "def initialize_projection_matrix(d_model, key):\n",
        "    \"\"\"Initializes the final linear projection matrix.\n",
        "\n",
        "    Args:\n",
        "        d_model: The model's embedding dimension.\n",
        "        key: A JAX PRNGKey for random number generation.\n",
        "\n",
        "    Returns:\n",
        "        The initialized projection matrix.\n",
        "    \"\"\"\n",
        "    final_projection = jax.random.normal(key, (d_model, d_model)) * jnp.sqrt(2 / (d_model + d_model))\n",
        "    return final_projection\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the projection matrix\n",
        "projection_key = jax.random.PRNGKey(4)  # Use a new key\n",
        "final_projection_matrix = initialize_projection_matrix(D_MODEL, projection_key)\n",
        "\n",
        "print(\"Shape of the final projection matrix:\", final_projection_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEO2r6dbvBlL",
        "outputId": "4f956665-3391-4683-b937-44bbb360e62d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the final projection matrix: (12288, 12288)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Apply Final Projection\n",
        "\n",
        "def apply_final_projection(concatenated_outputs, projection_matrix):\n",
        "    \"\"\"Applies the final linear projection to the concatenated attention outputs.\n",
        "\n",
        "    Args:\n",
        "        concatenated_outputs: The concatenated attention outputs from all heads.\n",
        "        projection_matrix: The final linear projection matrix.\n",
        "\n",
        "    Returns:\n",
        "        The output of the multi-head attention block.\n",
        "    \"\"\"\n",
        "    output = jnp.matmul(concatenated_outputs, projection_matrix)\n",
        "    return output\n",
        "\n",
        "# Apply the final projection\n",
        "final_output = apply_final_projection(concatenated_attention, final_projection_matrix)\n",
        "\n",
        "print(\"Shape of the final output:\", final_output.shape)\n",
        "print(\"First vector of the final output\", final_output[0])\n",
        "print(\"Last vector of the final output\", final_output[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnxtKPhiz_q3",
        "outputId": "14f9c3db-2ee6-4851-f6d9-2705ba8b0999"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the final output: (1024, 12288)\n",
            "First vector of the final output [-0.36862063  0.31341684  0.11350989 ...  0.12969272 -0.08512364\n",
            " -0.00763207]\n",
            "Last vector of the final output [ 0.2255859  -0.12203582  0.31405017 ...  0.31826627  0.03736952\n",
            " -0.49951586]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now eventually MLA reduces KV cache cause first you going to cache only one latent vector representation which is eventually compressed and smaller than normal KV cache.\n",
        "\n",
        "Yes MLA introduce new parameters in the system but this is great because now you have new parameters that are going to learn new behavoirs. and these 2 new matrices Wc and C. I dont count Wk and WV. since traditionaly also you would have K, V matrices to project the hidden states into the the D_k dimensions.\n",
        "\n",
        "For more math behind MLA check out deep seek paper https://arxiv.org/pdf/2405.04434"
      ],
      "metadata": {
        "id": "WWjps_YsxNZ7"
      }
    }
  ]
}